{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PhmeaWpsRb8W"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load datasets\n",
        "true_df = pd.read_csv('/content/politifact_real.csv')\n",
        "false_df = pd.read_csv('/content/politifact_real.csv')\n",
        "\n",
        "# Add labels\n",
        "true_df['label'] = 1\n",
        "false_df['label'] = 0\n",
        "\n",
        "# Combine the datasets\n",
        "df = pd.concat([true_df, false_df], ignore_index=True)\n",
        "\n",
        "# Save the combined dataset\n",
        "df.to_csv('combined.csv', index=False)\n"
      ],
      "metadata": {
        "id": "uhNQmJy7R4uV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import XLNetTokenizer\n",
        "\n",
        "# Load the combined dataset\n",
        "df = pd.read_csv('combined.csv')\n",
        "\n",
        "# Split the data into train and test sets\n",
        "train_df, test_df = train_test_split(df, test_size=0.5, random_state=42, stratify=df['label'])\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased')\n",
        "\n",
        "# Ensure all text entries are strings\n",
        "train_texts = train_df['text'].astype(str).tolist()\n",
        "test_texts = test_df['text'].astype(str).tolist()\n",
        "train_labels = train_df['label'].tolist()\n",
        "test_labels = test_df['label'].tolist()\n",
        "\n",
        "# Tokenize the text\n",
        "def tokenize_texts(texts):\n",
        "    return tokenizer(\n",
        "        texts,\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=512,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "\n",
        "train_encodings = tokenize_texts(train_texts)\n",
        "test_encodings = tokenize_texts(test_texts)"
      ],
      "metadata": {
        "id": "SoTQxW2zSgQU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import XLNetForSequenceClassification\n",
        "\n",
        "# Load the model\n",
        "model = XLNetForSequenceClassification.from_pretrained('xlnet-base-cased', num_labels=2)\n",
        "\n",
        "# Move the model to GPU if available\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D3mnLKjOSkhn",
        "outputId": "169a994d-d3fa-4d38-a8e2-b2e753b06fc4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['logits_proj.bias', 'logits_proj.weight', 'sequence_summary.summary.bias', 'sequence_summary.summary.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "XLNetForSequenceClassification(\n",
              "  (transformer): XLNetModel(\n",
              "    (word_embedding): Embedding(32000, 768)\n",
              "    (layer): ModuleList(\n",
              "      (0-11): 12 x XLNetLayer(\n",
              "        (rel_attn): XLNetRelativeAttention(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ff): XLNetFeedForward(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (activation_function): GELUActivation()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (sequence_summary): SequenceSummary(\n",
              "    (summary): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (activation): Tanh()\n",
              "    (first_dropout): Identity()\n",
              "    (last_dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (logits_proj): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "# Convert labels to tensors\n",
        "train_labels = torch.tensor(train_labels)\n",
        "test_labels = torch.tensor(test_labels)\n",
        "\n",
        "# Prepare DataLoader\n",
        "train_dataset = TensorDataset(train_encodings['input_ids'], train_encodings['attention_mask'], train_labels)\n",
        "test_dataset = TensorDataset(test_encodings['input_ids'], test_encodings['attention_mask'], test_labels)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
        "\n",
        "# Training parameters\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "total_steps = len(train_loader) * 4  # 4 epochs\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
        "\n",
        "# Training function\n",
        "def train(model, train_loader):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        input_ids, attention_mask, labels = [item.to(device) for item in batch]\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        total_loss += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "    return total_loss / len(train_loader)\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate(model, test_loader):\n",
        "    model.eval()\n",
        "    predictions, true_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            input_ids, attention_mask, labels = [item.to(device) for item in batch]\n",
        "            outputs = model(input_ids, attention_mask=attention_mask)\n",
        "            logits = outputs.logits\n",
        "            predictions.extend(torch.argmax(logits, dim=-1).cpu().numpy())\n",
        "            true_labels.extend(labels.cpu().numpy())\n",
        "    accuracy = accuracy_score(true_labels, predictions)\n",
        "    f1 = f1_score(true_labels, predictions, average='weighted')\n",
        "    return accuracy, f1\n",
        "\n",
        "# Training loop\n",
        "epochs = 6\n",
        "for epoch in range(epochs):\n",
        "    train_loss = train(model, train_loader)\n",
        "    print(f\"Epoch {epoch + 1}, Training loss: {train_loss}\")\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy, f1 = evaluate(model, test_loader)\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"F1 Score: {f1}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ekq1jPGgTORu",
        "outputId": "9f9ef42c-cfd5-4af4-851f-8f12e5a2895c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-17-d6e7a7426952>:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_labels = torch.tensor(train_labels)\n",
            "<ipython-input-17-d6e7a7426952>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  test_labels = torch.tensor(test_labels)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:521: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Training loss: 0.7081964910030365\n",
            "Epoch 2, Training loss: 0.703571999669075\n",
            "Epoch 3, Training loss: 0.7098169362545014\n",
            "Epoch 4, Training loss: 0.7003574883937835\n",
            "Epoch 5, Training loss: 0.7048386180400849\n",
            "Epoch 6, Training loss: 0.7068474835157394\n",
            "Accuracy: 0.4993726474278545\n",
            "F1 Score: 0.3326365082448302\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nlpaug\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xMstEGwfU8Ch",
        "outputId": "0481566b-c6bd-42e0-d0f7-6dc379336141"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nlpaug\n",
            "  Downloading nlpaug-1.1.11-py3-none-any.whl (410 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.5/410.5 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.16.2 in /usr/local/lib/python3.10/dist-packages (from nlpaug) (1.25.2)\n",
            "Requirement already satisfied: pandas>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from nlpaug) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.10/dist-packages (from nlpaug) (2.31.0)\n",
            "Requirement already satisfied: gdown>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from nlpaug) (5.1.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown>=4.0.0->nlpaug) (4.12.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown>=4.0.0->nlpaug) (3.14.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown>=4.0.0->nlpaug) (4.66.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.0->nlpaug) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.0->nlpaug) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.0->nlpaug) (2024.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (2024.2.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=1.2.0->nlpaug) (1.16.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown>=4.0.0->nlpaug) (2.5)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (1.7.1)\n",
            "Installing collected packages: nlpaug\n",
            "Successfully installed nlpaug-1.1.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import XLNetTokenizer\n",
        "import nlpaug.augmenter.word as naw\n",
        "import torch\n",
        "\n",
        "# Load the combined dataset\n",
        "df = pd.read_csv('combined.csv')\n",
        "\n",
        "# Ensure all text entries are strings\n",
        "df['text'] = df['text'].astype(str)\n",
        "\n",
        "# Data augmentation function\n",
        "def augment_text(text):\n",
        "    aug = naw.SynonymAug(aug_src='wordnet')\n",
        "    return aug.augment(text)\n",
        "\n",
        "# Augment the dataset\n",
        "augmented_texts = df['text'].apply(augment_text)\n",
        "augmented_labels = df['label']\n",
        "\n",
        "augmented_df = pd.DataFrame({'text': augmented_texts, 'label': augmented_labels})\n",
        "df = pd.concat([df, augmented_df])\n",
        "\n",
        "# Filter out invalid text entries\n",
        "df = df[df['text'].apply(lambda x: isinstance(x, str) and x.strip().lower() != 'nan')]\n",
        "\n",
        "# Split the data into train and test sets\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['label'])\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased')\n",
        "\n",
        "# Tokenize the text\n",
        "def tokenize_texts(texts):\n",
        "    return tokenizer(\n",
        "        texts,  # Ensure texts is a list of strings\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=512,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "\n",
        "train_texts = train_df['text'].tolist()\n",
        "test_texts = test_df['text'].tolist()\n",
        "train_labels = train_df['label'].tolist()\n",
        "test_labels = test_df['label'].tolist()\n",
        "\n",
        "# Ensure labels match the texts\n",
        "assert len(train_texts) == len(train_labels), \"Mismatch between number of training texts and labels.\"\n",
        "assert len(test_texts) == len(test_labels), \"Mismatch between number of testing texts and labels.\"\n",
        "\n",
        "# Debug: Print a few examples to check the format\n",
        "print(\"Sample train texts:\", train_texts[:5])\n",
        "print(\"Sample test texts:\", test_texts[:5])\n",
        "\n",
        "# Tokenize the texts\n",
        "train_encodings = tokenize_texts(train_texts)\n",
        "test_encodings = tokenize_texts(test_texts)\n",
        "\n",
        "# Convert labels to tensors\n",
        "train_labels = torch.tensor(train_labels)\n",
        "test_labels = torch.tensor(test_labels)\n",
        "\n",
        "from transformers import XLNetForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "# Prepare DataLoader\n",
        "train_dataset = TensorDataset(train_encodings['input_ids'], train_encodings['attention_mask'], train_labels)\n",
        "test_dataset = TensorDataset(test_encodings['input_ids'], test_encodings['attention_mask'], test_labels)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "# Load the model\n",
        "model = XLNetForSequenceClassification.from_pretrained('xlnet-base-cased', num_labels=2)\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "model.to(device)\n",
        "\n",
        "# Training parameters\n",
        "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
        "total_steps = len(train_loader) * 6  # Increase to 6 epochs\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
        "\n",
        "# Training function\n",
        "def train(model, train_loader):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        input_ids, attention_mask, labels = [item.to(device) for item in batch]\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        total_loss += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "    return total_loss / len(train_loader)\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate(model, test_loader):\n",
        "    model.eval()\n",
        "    predictions, true_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            input_ids, attention_mask, labels = [item.to(device) for item in batch]\n",
        "            outputs = model(input_ids, attention_mask=attention_mask)\n",
        "            logits = outputs.logits\n",
        "            predictions.extend(torch.argmax(logits, dim=-1).cpu().numpy())\n",
        "            true_labels.extend(labels.cpu().numpy())\n",
        "    accuracy = accuracy_score(true_labels, predictions)\n",
        "    f1 = f1_score(true_labels, predictions, average='weighted')\n",
        "    return accuracy, f1\n",
        "\n",
        "# Training loop\n",
        "epochs = 6\n",
        "for epoch in range(epochs):\n",
        "    train_loss = train(model, train_loader)\n",
        "    print(f\"Epoch {epoch + 1}, Training loss: {train_loss}\")\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy, f1 = evaluate(model, test_loader)\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"F1 Score: {f1}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ua4eCKI0f3mj",
        "outputId": "2963db4f-6f29-411b-89f1-744ecccf9bcb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample train texts: ['Remarks by President Trump in a Listening Session on Healthcare', 'The Committee on Energy and Commerce', 'Palin tries new tactic to unload hated jet: Gov. Sarah Palin background', 'The Des Moines Register', 'International Energy Statistics']\n",
            "Sample test texts: ['Political Figures: C (2)', 'The CNN Miami Republican debate transcript, annotated', 'Analysis of the 2008 Presidential Candidates’ Tax Plans', 'Obama addresses key concerns for Floridians', 'The Democratic Debate in Cleveland']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['logits_proj.bias', 'logits_proj.weight', 'sequence_summary.summary.bias', 'sequence_summary.summary.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:521: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Training loss: 0.7245282909226796\n",
            "Epoch 2, Training loss: 0.7042736193490406\n",
            "Epoch 3, Training loss: 0.704368982050154\n",
            "Epoch 4, Training loss: 0.7114719133528452\n",
            "Epoch 5, Training loss: 0.7001823461244977\n",
            "Epoch 6, Training loss: 0.7046851203555152\n",
            "Accuracy: 0.456\n",
            "F1 Score: 0.39733408961996597\n"
          ]
        }
      ]
    }
  ]
}